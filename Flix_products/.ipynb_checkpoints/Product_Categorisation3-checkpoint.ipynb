{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pandas as pd, mysql.connector, nltk\n",
    "import xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first file which has product titles and brand categoriws\n",
    "df1 = pd.read_csv('\\\\Users\\\\Dan\\\\Documents\\\\Python\\\\Data\\\\Flix_products\\\\product_title.csv', engine='python')\n",
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load second product file and load only the necessary columns\n",
    "products_complete = pd.read_csv('\\\\Users\\\\Dan\\\\Documents\\\\Python\\\\Data\\\\Flix_products\\\\products.csv', sep='\\t')\n",
    "products= products_complete[['product_id','product_title','mpn_list','manufacturer_id', 'brand_title',\n",
    "                            'flix_parent_category', 'flix_subCategory1', 'flix_subCategory2',\n",
    "                            'benchmark_category', 'benchmark_category2']]\n",
    "products.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two products file with an inner join to keep only the common products from both\n",
    "products = pd.merge(products, df1, on='product_id', how='inner')\n",
    "products.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many products have unassigned categories\n",
    "products['benchmark_category2'].isna().sum(), products['benchmark_category2'].isna().sum()/len(products.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the unassugned category products\n",
    "products.dropna(subset=['benchmark_category2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the distribution of number of products and brands by product categories\n",
    "group_products = products.groupby('benchmark_category2')['product_id'].count()\n",
    "group_brand  = products.groupby('benchmark_category2')['brand_title'].nunique()\n",
    "plt.figure(figsize=(15,5))\n",
    "ax = sns.scatterplot(x=group_products, y=group_brand)\n",
    "ax.set_ylabel('# of brands')\n",
    "ax.set_xlabel('# of products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at the top 30 categories by number of products\n",
    "products.groupby('benchmark_category2')['product_id'].count().sort_values().tail(30).plot.barh(figsize=(8,8), color='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets subset by top 50 categories and use this data for further analysis\n",
    "top_Ncats = products.groupby('benchmark_category2')['product_id'].count().nlargest(50).keys()\n",
    "prod_Ncats = products[products['benchmark_category2'].isin(top_Ncats)].reset_index(drop=True)\n",
    "prod_Ncats.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portion of overall data in the top 50 categories\n",
    "len(prod_Ncats)/len(products)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Categorisation with product titles and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_Ncats['Categories'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the pipes '|' in the Categories\n",
    "prod_Ncats['Categories'] = prod_Ncats['Categories'].str.replace('|', '')\n",
    "prod_Ncats['Categories'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets concatenate the two columns : product titles and categories\n",
    "prod_Ncats['title&cat']=np.nan\n",
    "for i in range(len(prod_Ncats)):\n",
    "    if pd.isna(prod_Ncats.loc[i, 'product_title']):\n",
    "        prod_Ncats.loc[i, 'title&cat'] =  prod_Ncats.loc[i, 'title'] + ' ' + prod_Ncats.loc[i, 'Categories']\n",
    "    else:\n",
    "        prod_Ncats.loc[i,'title&cat'] =  prod_Ncats.loc[i, 'product_title'] + ' ' + prod_Ncats.loc[i, 'Categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_Ncats.to_csv('prod_50cats.csv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_Ncats = pd.read_csv('prod_50cats.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for empty strings in the two columns \n",
    "#prod_15cats[prod_15cats[['product_title','Categories']].isnull().values.any(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to tokenize the whole doc and form a vocabulary\n",
    "\n",
    "# clean texts\n",
    "def tokenize_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = lemmatized_output = [' '.join([lemmatizer.lemmatize(w) for w in tokens])]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load text and add to vocab\n",
    "def add_to_vocab(column, vocab):\n",
    "    for i in range(len(column)):\n",
    "        tokens = tokenize_doc(column[i])\n",
    "        vocab.update(tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the vocabulary after tokenization of the whole doc\n",
    "vocab = Counter()\n",
    "add_to_vocab(prod_Ncats['title&cat'], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    HP EliteBook 840 G1 Notebook PC Laptops And Hy...\n",
       "1                         Speedlight SB-N7 Speedlights\n",
       "2    Electrolux EFC60465OX 60 cm Stainless Steel Ch...\n",
       "3    Electrolux EFC90468OX 90 cm Stainless Steel Ch...\n",
       "4            Oven EVY9841AOX Electrolux  NonStar  Oven\n",
       "Name: title&cat, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_Ncats['title&cat'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hp', 81516), ('pc', 34871), ('notebook', 24464), ('series', 20676), ('pavilion', 11978), ('home', 9793), ('desktop', 9730), ('pcs', 9598), ('workstation', 7968), ('compaq', 7286), ('mobile', 7160), ('philips', 7101), ('consumer', 6220), ('tv', 6122), ('ink', 5807), ('elitebook', 5680), ('led', 5631), ('probook', 5555), ('printer', 5503), ('cartridges', 4748)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11832, None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of the vocab and the most occuring words\n",
    "len(vocab), print(vocab.most_common(20))\n",
    "#print(vocab.most_common()[:len(vocab)-100:-1])  # to obtain the lower list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 7373\n"
     ]
    }
   ],
   "source": [
    "# remove tokens with just one occurence to reduce the vocab size\n",
    "tokens = [k for k,c in vocab.items() if c > 1]\n",
    "print('Vocabulary size: {}'.format( len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the vocabulary to a file\n",
    "def save_vocab(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w', encoding=\"utf-8\")\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "# save the vocabulary and tokens\n",
    "save_vocab(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', encoding=\"utf-8\") as f:\n",
    "    lines = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize each row\n",
    "def tokenize_row(column, vocab):\n",
    "    tokenz = []\n",
    "    for i in range(len(column)):\n",
    "        tokens = tokenize_doc(column[i])\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "        tokenz.append(tokens)\n",
    "    return tokenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [hp, elitebook, notebook, pc, laptops, hybrids...\n",
       "1                            [speedlight, speedlights]\n",
       "2    [electrolux, cm, stainless, steel, chimney, de...\n",
       "3    [electrolux, cm, stainless, steel, chimney, de...\n",
       "4                    [oven, electrolux, nonstar, oven]\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize each row\n",
    "prod_Ncats['tokens'] = tokenize_row(prod_Ncats['title&cat'], vocab)\n",
    "prod_Ncats['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of token lengths per product\n",
    "token_length = []\n",
    "for i in range(len(prod_Ncats['tokens'])):\n",
    "    token_length.append(len(prod_Ncats['tokens'][i]))\n",
    "    \n",
    "plt.figure(figsize=(10,5))   \n",
    "sns.distplot(token_length, bins=50, kde=False, rug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate example\n",
    "# for ele in enumerate(prod_Ncats['tokens'][1]): \n",
    "#     print(ele)\n",
    "# for index, ele in enumerate(prod_Ncats['tokens'][1]): \n",
    "#     print (index,ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    hp elitebook notebook pc laptops hybrids busin...\n",
       "1                               speedlight speedlights\n",
       "2    electrolux cm stainless steel chimney design h...\n",
       "3    electrolux cm stainless steel chimney design h...\n",
       "4                         oven electrolux nonstar oven\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert tokens in the list to texts to make it vectorizing freindly\n",
    "prod_Ncats['tokens']=[\" \".join(tokens) for tokens in prod_Ncats['tokens'].values]\n",
    "prod_Ncats['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot bar-chart of groupby agg counts\n",
    "def bar_plot(df,group_column, agg):\n",
    "    df.groupby(group_column)[agg].count().sort_values(ascending=False).plot.bar(figsize=(12,6), color='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the count of products by categories\n",
    "bar_plot(prod_Ncats, 'benchmark_category2' ,'product_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test data split with stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test set with stratified sampling\n",
    "model_data = prod_Ncats.copy()\n",
    "y = model_data.pop('benchmark_category2')\n",
    "x = model_data['tokens']\n",
    "x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.30, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse transformation of LabelEncoder\n",
    "#encoder.inverse_transform(y_test)\n",
    "#print(list(encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the TF-IDF features and transform the tran and test datasets\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{2,}', stop_words='english', max_features=7000)\n",
    "tfidf_vect.fit(prod_Ncats['tokens'])\n",
    "x_train_tfidf =  tfidf_vect.transform(x_train)\n",
    "x_test_tfidf =  tfidf_vect.transform(x_test)\n",
    "len(tfidf_vect.vocabulary_), x_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_tfidf[1,:150].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot-encoding - Count Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the CountVector features and transform the tran and test datasets\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{2,}', stop_words='english')\n",
    "count_vect.fit(prod_Ncats['tokens'])\n",
    "x_train_CV =  count_vect.transform(x_train)\n",
    "x_test_CV =  count_vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document term matrix\n",
    "print(x_train_CV[20:25,1:30].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.get_feature_names()[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_model():\n",
    "    def __init__(self,classifier, train_data, test_data):\n",
    "        self.train = classifier.fit(train_data, y_train)\n",
    "        self.predict = classifier.predict(test_data)\n",
    "        self.cm = confusion_matrix(y_test, self.predict)\n",
    "        self.precision = np.diag(self.cm) / np.sum(self.cm, axis = 0)\n",
    "        self.recall = np.diag(self.cm) / np.sum(self.cm, axis = 1)  \n",
    "        \n",
    "    def return_metrics(self):\n",
    "        print('Accuracy: {}'.format(metrics.accuracy_score(self.predict, y_test)))\n",
    "        print('Average Precision: {}'.format(np.average(self.precision)))\n",
    "        print('Average Recall: {}'.format(np.average(self.recall)))\n",
    "    \n",
    "    def return_predictions(self):\n",
    "        return self.predict        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot the count of wrong predictions by categories\n",
    "def plot_wrong_predictions(model):\n",
    "    # extract the predictions\n",
    "    predicted = model.return_predictions()\n",
    "    # subset the training data rows from the main dataframe\n",
    "    train_data = prod_Ncats.iloc[list(x_test.index.values),:]\n",
    "    # add the predicted columns to the train_data\n",
    "    train_data['predicted'] = encoder.inverse_transform(predicted)\n",
    "    # extract the wrong predictions\n",
    "    wrong_predictions = train_data[train_data['benchmark_category2'] != train_data['predicted']]\n",
    "    warnings.filterwarnings('ignore')\n",
    "    # plot a count of the wrong categories\n",
    "    #bar_plot(wrong_predictions, 'benchmark_category2' ,'product_id')  \n",
    "    ax = wrong_predictions.groupby('benchmark_category2')['product_id'].nunique().sort_values(ascending=False).plot.bar(color='purple')\n",
    "    ax.set_xlabel('categories')\n",
    "    ax.set_ylabel('number of products')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models on the TF-IDF features\n",
    "NB_tfidf = train_model(naive_bayes.MultinomialNB(), x_train_tfidf, x_test_tfidf)   # Naive Bayes\n",
    "LR_tfidf = train_model(linear_model.LogisticRegression(), x_train_tfidf, x_test_tfidf)  # Logistic Regression\n",
    "RF_tfidf = train_model(ensemble.RandomForestClassifier(), x_train_tfidf, x_test_tfidf)  # Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure the predictions\n",
    "NB_tfidf.return_metrics()\n",
    "LR_tfidf.return_metrics()\n",
    "RF_tfidf.return_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the wrong predictions between two models\n",
    "f, axs = plt.subplots(1,2,figsize=(18,6))\n",
    "plt.subplot(1,2,1)\n",
    "plot_wrong_predictions(NB_tfidf)\n",
    "plt.title('NB_tfidf wrong prdictions')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plot_wrong_predictions(RF_tfidf)\n",
    "plt.title('RF_tfidf wrong prdictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models on the CountVector features\n",
    "NB_CV = train_model(naive_bayes.MultinomialNB(), x_train_CV, x_test_CV)   # Naive Bayes\n",
    "LR_CV = train_model(linear_model.LogisticRegression(), x_train_CV, x_test_CV)  # Logistic Regression\n",
    "RF_CV = train_model(ensemble.RandomForestClassifier(), x_train_CV, x_test_CV)  # Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure the predictions\n",
    "NB_CV.return_metrics()\n",
    "LR_CV.return_metrics()\n",
    "RF_CV.return_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the wrong predictions between two models\n",
    "f, axs = plt.subplots(1,2,figsize=(18,6))\n",
    "plt.subplot(1,2,1)\n",
    "plot_wrong_predictions(LR_CV)\n",
    "plt.title('LR_CV wrong prdictions')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plot_wrong_predictions(RF_CV)\n",
    "plt.title('RF_CV wrong prdictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the predictions\n",
    "# predicted = NB_tfidf.return_predictions()\n",
    "# # transform the predictions to the original categories\n",
    "# encoder.inverse_transform(predicted)\n",
    "# # # subset the training data rows from the main dataframe\n",
    "# # train_data = prod_Ncats.iloc[list(x_test.index.values),:]\n",
    "# # # add the predicted columns to the train_data\n",
    "# # train_data['predicted'] = encoder.inverse_transform(predicted)\n",
    "# # # extract the wrong predictions\n",
    "# # wrong_predictions = train_data[train_data['benchmark_category2'] != train_data['predicted']]\n",
    "# # # plot a count of the wrong categories\n",
    "# # bar_plot(wrong_predictions, 'benchmark_category2' ,'product_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling - Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63349,), 7373, 11832)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_Ncats['tokens'].shape, len(tokens), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hp</th>\n",
       "      <th>elitebook</th>\n",
       "      <th>notebook</th>\n",
       "      <th>pc</th>\n",
       "      <th>laptops</th>\n",
       "      <th>hybrids</th>\n",
       "      <th>business</th>\n",
       "      <th>laptop</th>\n",
       "      <th>pcs</th>\n",
       "      <th>series</th>\n",
       "      <th>...</th>\n",
       "      <th>lanyard</th>\n",
       "      <th>ergofit</th>\n",
       "      <th>error</th>\n",
       "      <th>unable</th>\n",
       "      <th>compute</th>\n",
       "      <th>smartcook</th>\n",
       "      <th>mattel</th>\n",
       "      <th>ftz</th>\n",
       "      <th>freshcare</th>\n",
       "      <th>wfe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7373 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hp  elitebook  notebook  pc  laptops  hybrids  business  laptop  pcs  \\\n",
       "0   4          4         4   3        1        1         1       1    2   \n",
       "1   0          0         0   0        0        0         0       0    0   \n",
       "2   0          0         0   0        0        0         0       0    0   \n",
       "3   0          0         0   0        0        0         0       0    0   \n",
       "4   0          0         0   0        0        0         0       0    0   \n",
       "\n",
       "   series  ...  lanyard  ergofit  error  unable  compute  smartcook  mattel  \\\n",
       "0       1  ...        0        0      0       0        0          0       0   \n",
       "1       0  ...        0        0      0       0        0          0       0   \n",
       "2       0  ...        0        0      0       0        0          0       0   \n",
       "3       0  ...        0        0      0       0        0          0       0   \n",
       "4       0  ...        0        0      0       0        0          0       0   \n",
       "\n",
       "   ftz  freshcare  wfe  \n",
       "0    0          0    0  \n",
       "1    0          0    0  \n",
       "2    0          0    0  \n",
       "3    0          0    0  \n",
       "4    0          0    0  \n",
       "\n",
       "[5 rows x 7373 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a Term Matrix with CountVectorizer\n",
    "cv = CountVectorizer(stop_words='english', vocabulary=list(tokens))\n",
    "data_cv = cv.fit_transform(prod_Ncats['tokens'])\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = prod_Ncats.index\n",
    "data_dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>63339</th>\n",
       "      <th>63340</th>\n",
       "      <th>63341</th>\n",
       "      <th>63342</th>\n",
       "      <th>63343</th>\n",
       "      <th>63344</th>\n",
       "      <th>63345</th>\n",
       "      <th>63346</th>\n",
       "      <th>63347</th>\n",
       "      <th>63348</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elitebook</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notebook</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pc</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laptops</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63349 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0      1      2      3      4      5      6      7      8      \\\n",
       "hp             4      0      0      0      0      0      0      0      0   \n",
       "elitebook      4      0      0      0      0      0      0      0      0   \n",
       "notebook       4      0      0      0      0      0      0      0      0   \n",
       "pc             3      0      0      0      0      0      0      0      0   \n",
       "laptops        1      0      0      0      0      0      0      0      0   \n",
       "\n",
       "           9      ...  63339  63340  63341  63342  63343  63344  63345  63346  \\\n",
       "hp             0  ...      0      0      0      0      0      0      0      0   \n",
       "elitebook      0  ...      0      0      0      0      0      0      0      0   \n",
       "notebook       0  ...      0      0      0      0      0      0      0      0   \n",
       "pc             0  ...      0      0      0      0      0      0      0      0   \n",
       "laptops        0  ...      0      0      0      0      0      0      0      0   \n",
       "\n",
       "           63347  63348  \n",
       "hp             0      0  \n",
       "elitebook      0      0  \n",
       "notebook       0      0  \n",
       "pc             0      0  \n",
       "laptops        0      0  \n",
       "\n",
       "[5 rows x 63349 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transpose the Term Matrix to a Term-Document Matrix\n",
    "tdm = data_dtm.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the Term-Document Matrix into a gensim format corpus, df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 4), (1, 4), (2, 4), (3, 3), (4, 1), (5, 1), (6, 1), (7, 1), (8, 2), (9, 1)], [(10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(list(corpus)[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dictionary needed for LDA modelling\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('hp', 4), ('elitebook', 4), ('notebook', 4), ('pc', 3), ('laptops', 1), ('hybrids', 1), ('business', 1), ('laptop', 1), ('pcs', 2), ('series', 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "print([[(id2word[id], freq) for id, freq in cp] for cp in list(corpus)[:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"epson\" + 0.019*\"cameras\" + 0.018*\"steam\" + 0.015*\"appliances\" + 0.012*\"storage\" + 0.011*\"vacuum\" + 0.010*\"tefal\" + 0.010*\"food\" + 0.009*\"kitchen\" + 0.009*\"kmix\"'),\n",
       " (1,\n",
       "  '0.041*\"mobile\" + 0.037*\"hp\" + 0.032*\"printer\" + 0.029*\"printers\" + 0.029*\"accessories\" + 0.026*\"multifunction\" + 0.026*\"series\" + 0.024*\"laser\" + 0.023*\"lenovo\" + 0.022*\"galaxy\"'),\n",
       " (2,\n",
       "  '0.052*\"tv\" + 0.023*\"hd\" + 0.023*\"monitors\" + 0.021*\"aeg\" + 0.018*\"cm\" + 0.018*\"oven\" + 0.017*\"home\" + 0.016*\"tvs\" + 0.016*\"nonstar\" + 0.016*\"ultra\"'),\n",
       " (3,\n",
       "  '0.231*\"hp\" + 0.102*\"pc\" + 0.074*\"notebook\" + 0.066*\"pcs\" + 0.053*\"series\" + 0.049*\"desktop\" + 0.037*\"home\" + 0.037*\"pavilion\" + 0.033*\"laptop\" + 0.024*\"laptops\"'),\n",
       " (4,\n",
       "  '0.063*\"workstation\" + 0.037*\"mobile\" + 0.035*\"lighting\" + 0.030*\"led\" + 0.029*\"headphones\" + 0.028*\"philips\" + 0.026*\"home\" + 0.025*\"zbook\" + 0.021*\"audio\" + 0.021*\"consumer\"')]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_5 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, passes=20)\n",
    "lda_5.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a POS tokenizer that only keeps nouns\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the nouns from the data\n",
    "data_nouns = pd.DataFrame(prod_Ncats['tokens'].apply(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Term Matrix with CountVectorizer\n",
    "cv_nouns = CountVectorizer(stop_words='english')\n",
    "#cv_nouns._validate_vocabulary()\n",
    "data_cv_nouns = cv_nouns.fit_transform(data_nouns['tokens'])\n",
    "data_cv_nouns_dtm = pd.DataFrame(data_cv_nouns.toarray(), columns=cv_nouns.get_feature_names())\n",
    "data_cv_nouns_dtm.index = data_nouns.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_cv_nouns_dtm.transpose()))\n",
    "# Create the vocabulary dictionary\n",
    "id2word_n = dict((v, k) for k, v in cv_nouns.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  '0.087*\"combination\" + 0.086*\"stand\" + 0.078*\"whirlpool\" + 0.064*\"print\" + 0.047*\"standing\" + 0.037*\"connectivity\" + 0.036*\"hpe\" + 0.034*\"hpdefault\" + 0.031*\"unit\" + 0.020*\"link\"'),\n",
       " (27,\n",
       "  '0.211*\"hp\" + 0.202*\"desktop\" + 0.125*\"pc\" + 0.115*\"pcs\" + 0.084*\"business\" + 0.073*\"series\" + 0.061*\"workstations\" + 0.048*\"prodesk\" + 0.013*\"server\" + 0.009*\"elitedesk\"'),\n",
       " (16,\n",
       "  '0.074*\"food\" + 0.059*\"hand\" + 0.046*\"kitchen\" + 0.040*\"blenders\" + 0.039*\"blender\" + 0.038*\"ovens\" + 0.034*\"mixer\" + 0.034*\"kmix\" + 0.028*\"processor\" + 0.028*\"kenwood\"'),\n",
       " (6,\n",
       "  '0.371*\"epson\" + 0.070*\"client\" + 0.060*\"hdr\" + 0.054*\"keyboard\" + 0.053*\"singlepack\" + 0.045*\"adapter\" + 0.042*\"base\" + 0.041*\"keyboards\" + 0.039*\"model\" + 0.016*\"cyan\"'),\n",
       " (22,\n",
       "  '0.124*\"corsair\" + 0.103*\"phone\" + 0.100*\"support\" + 0.081*\"cases\" + 0.074*\"samsung\" + 0.069*\"case\" + 0.058*\"mobile\" + 0.037*\"view\" + 0.034*\"colour\" + 0.030*\"charger\"'),\n",
       " (17,\n",
       "  '0.227*\"ink\" + 0.155*\"cartridges\" + 0.122*\"supplies\" + 0.078*\"cartridge\" + 0.068*\"refrigerators\" + 0.056*\"hp\" + 0.037*\"inktonerpaperprinter\" + 0.029*\"magenta\" + 0.024*\"carpet\" + 0.018*\"inksprintheads\"'),\n",
       " (11,\n",
       "  '0.232*\"form\" + 0.229*\"factor\" + 0.150*\"lego\" + 0.035*\"media\" + 0.024*\"cylinder\" + 0.019*\"ice\" + 0.018*\"porsche\" + 0.015*\"gaming\" + 0.009*\"conf\" + 0.009*\"nikon\"'),\n",
       " (32,\n",
       "  '0.135*\"appliances\" + 0.118*\"steam\" + 0.051*\"tefal\" + 0.046*\"iron\" + 0.039*\"cookers\" + 0.032*\"irons\" + 0.031*\"technology\" + 0.030*\"consumer\" + 0.028*\"care\" + 0.025*\"household\"'),\n",
       " (7,\n",
       "  '0.174*\"pavilion\" + 0.146*\"hp\" + 0.141*\"pc\" + 0.122*\"series\" + 0.116*\"notebook\" + 0.073*\"home\" + 0.067*\"pcs\" + 0.038*\"laptops\" + 0.036*\"envy\" + 0.023*\"star\"'),\n",
       " (4,\n",
       "  '0.142*\"wireless\" + 0.116*\"speakers\" + 0.103*\"wall\" + 0.066*\"sound\" + 0.059*\"speaker\" + 0.056*\"vision\" + 0.038*\"consumer\" + 0.034*\"audio\" + 0.033*\"philips\" + 0.031*\"entertainment\"'),\n",
       " (15,\n",
       "  '0.165*\"vacuum\" + 0.128*\"power\" + 0.125*\"cleaners\" + 0.052*\"microsoftdefault\" + 0.037*\"cordless\" + 0.037*\"radio\" + 0.027*\"surface\" + 0.019*\"directdefault\" + 0.016*\"vax\" + 0.016*\"windows\"'),\n",
       " (25,\n",
       "  '0.252*\"headphones\" + 0.091*\"machines\" + 0.089*\"machine\" + 0.058*\"coffee\" + 0.040*\"consumer\" + 0.037*\"range\" + 0.030*\"philips\" + 0.028*\"sports\" + 0.027*\"entertainment\" + 0.025*\"lifestyle\"'),\n",
       " (9,\n",
       "  '0.288*\"pc\" + 0.198*\"hp\" + 0.160*\"compaq\" + 0.079*\"omen\" + 0.042*\"star\" + 0.040*\"series\" + 0.030*\"notebook\" + 0.027*\"value\" + 0.024*\"presario\" + 0.022*\"energy\"'),\n",
       " (14,\n",
       "  '0.236*\"hp\" + 0.170*\"pc\" + 0.158*\"notebook\" + 0.107*\"pcs\" + 0.082*\"hybrids\" + 0.067*\"elitebook\" + 0.065*\"probook\" + 0.037*\"series\" + 0.034*\"business\" + 0.011*\"star\"'),\n",
       " (31,\n",
       "  '0.114*\"toner\" + 0.108*\"paper\" + 0.091*\"supplies\" + 0.073*\"laser\" + 0.062*\"hp\" + 0.058*\"laserjet\" + 0.047*\"kits\" + 0.040*\"printing\" + 0.039*\"cartridges\" + 0.038*\"color\"'),\n",
       " (19,\n",
       "  '0.278*\"workstation\" + 0.265*\"hp\" + 0.123*\"workstations\" + 0.101*\"zbook\" + 0.077*\"desktops\" + 0.052*\"series\" + 0.029*\"mobile\" + 0.019*\"tower\" + 0.011*\"star\" + 0.007*\"energy\"'),\n",
       " (30,\n",
       "  '0.090*\"car\" + 0.082*\"pack\" + 0.063*\"makers\" + 0.057*\"philips\" + 0.056*\"bulbs\" + 0.048*\"lamps\" + 0.043*\"lights\" + 0.040*\"bulb\" + 0.033*\"glass\" + 0.033*\"type\"'),\n",
       " (34,\n",
       "  '0.117*\"builtin\" + 0.114*\"aeg\" + 0.108*\"hob\" + 0.098*\"nonstar\" + 0.056*\"cm\" + 0.055*\"steel\" + 0.043*\"gas\" + 0.035*\"stainless\" + 0.030*\"edition\" + 0.030*\"star\"'),\n",
       " (8,\n",
       "  '0.119*\"premium\" + 0.093*\"control\" + 0.067*\"lacie\" + 0.044*\"mit\" + 0.038*\"thinkcentre\" + 0.028*\"cd\" + 0.027*\"tape\" + 0.027*\"fan\" + 0.020*\"barracuda\" + 0.017*\"grip\"'),\n",
       " (23,\n",
       "  '0.090*\"hardware\" + 0.077*\"day\" + 0.053*\"services\" + 0.034*\"sensor\" + 0.033*\"microwaves\" + 0.032*\"warranty\" + 0.030*\"goprodefault\" + 0.027*\"care\" + 0.026*\"hw\" + 0.025*\"service\"')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 40 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=40, id2word=id2word_n, passes=100)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-e74e357086b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Let's take a look at which topics each transcript contains\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcorpus_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mldan\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpusn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus_transformed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_cv_nouns_dtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-103-e74e357086b0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Let's take a look at which topics each transcript contains\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcorpus_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mldan\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpusn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus_transformed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_cv_nouns_dtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 1)"
     ]
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldan[corpusn]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_cv_nouns_dtm.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.174*hp + 0.131*pc + 0.072*notebook + 0.057*series + 0.047*pavilion + 0.035*pcs + 0.023*workstation + 0.023*probook + 0.022*compaq + 0.019*desktop + 0.015*elitebook + 0.014*home + 0.014*form + 0.014*factor + 0.014*energy + 0.014*star + 0.013*workstations + 0.010*entertainment + 0.010*business + 0.009*hybrids'),\n",
       " (1,\n",
       "  '0.040*tv + 0.027*monitors + 0.021*hp + 0.020*home + 0.019*philips + 0.017*hd + 0.016*consumer + 0.015*series + 0.015*pc + 0.011*monitor + 0.010*entertainment + 0.010*tvs + 0.009*cm + 0.009*vision + 0.008*televisions + 0.008*accessories + 0.007*audio + 0.007*notebook + 0.006*speakers + 0.006*video'),\n",
       " (2,\n",
       "  '0.043*supplies + 0.043*hp + 0.040*cartridges + 0.031*ink + 0.026*toner + 0.020*printer + 0.019*laser + 0.015*inktonerpaperprinter + 0.015*color + 0.015*laserjet + 0.010*printing + 0.010*kits + 0.009*cartridge + 0.009*pc + 0.009*series + 0.008*paper + 0.006*printers + 0.006*print + 0.006*epson + 0.005*notebook'),\n",
       " (3,\n",
       "  '0.059*hp + 0.053*printer + 0.047*series + 0.046*printers + 0.026*color + 0.020*laser + 0.017*multifunction + 0.016*laserjet + 0.016*allinone + 0.011*pc + 0.010*deskjet + 0.009*inkjet + 0.009*office + 0.006*notebook + 0.005*ink + 0.004*photo + 0.004*eallinone + 0.004*pavilion + 0.004*samsung + 0.004*enterprise'),\n",
       " (4,\n",
       "  '0.042*hp + 0.027*support + 0.023*year + 0.021*hardware + 0.020*business + 0.019*day + 0.012*pc + 0.008*series + 0.008*services + 0.006*notebook + 0.005*warranty + 0.005*post + 0.004*printer + 0.004*service + 0.004*laserjet + 0.004*hw + 0.004*pavilion + 0.004*care + 0.003*printers + 0.003*pcs'),\n",
       " (5,\n",
       "  '0.023*philips + 0.022*consumer + 0.019*luminaires + 0.018*hp + 0.013*fixture + 0.011*home + 0.010*pc + 0.009*spot + 0.008*wall + 0.006*spots + 0.006*series + 0.006*deco + 0.006*lamps + 0.005*notebook + 0.005*light + 0.005*livingroom + 0.004*myliving + 0.004*ceiling + 0.004*suspension + 0.004*bulb'),\n",
       " (6,\n",
       "  '0.021*steam + 0.014*hp + 0.009*consumer + 0.009*philips + 0.009*iron + 0.008*pc + 0.008*appliances + 0.008*series + 0.007*care + 0.006*products + 0.006*household + 0.005*irons + 0.005*vacuum + 0.005*generator + 0.004*accessories + 0.004*notebook + 0.004*garment + 0.004*generators + 0.004*cleaners + 0.004*home'),\n",
       " (7,\n",
       "  '0.034*headphones + 0.020*hp + 0.012*pc + 0.011*accessories + 0.008*phone + 0.008*consumer + 0.008*philips + 0.007*series + 0.007*entertainment + 0.006*notebook + 0.006*lifestyle + 0.004*headsets + 0.004*pavilion + 0.004*vision + 0.004*home + 0.004*galaxy + 0.003*cover + 0.003*pcs + 0.003*phones + 0.003*workstation'),\n",
       " (8,\n",
       "  '0.029*hp + 0.015*pc + 0.009*series + 0.008*notebook + 0.006*support + 0.006*year + 0.005*business + 0.005*hardware + 0.005*pavilion + 0.005*pcs + 0.004*day + 0.004*home + 0.003*workstation + 0.003*printer + 0.003*philips + 0.003*consumer + 0.003*services + 0.003*desktop + 0.003*tv + 0.002*elitebook'),\n",
       " (9,\n",
       "  '0.032*hp + 0.020*pc + 0.011*series + 0.010*notebook + 0.007*pcs + 0.006*pavilion + 0.005*workstation + 0.004*home + 0.004*desktop + 0.003*business + 0.003*energy + 0.003*probook + 0.003*compaq + 0.003*workstations + 0.003*philips + 0.003*elitebook + 0.002*star + 0.002*consumer + 0.002*tv + 0.002*accessories'),\n",
       " (10,\n",
       "  '0.029*hp + 0.019*pc + 0.011*series + 0.010*notebook + 0.007*pavilion + 0.005*pcs + 0.004*workstation + 0.004*home + 0.003*energy + 0.003*compaq + 0.003*probook + 0.003*desktop + 0.003*philips + 0.003*printer + 0.003*elitebook + 0.002*consumer + 0.002*ink + 0.002*star + 0.002*tv + 0.002*supplies'),\n",
       " (11,\n",
       "  '0.023*hp + 0.014*pc + 0.008*series + 0.007*notebook + 0.005*freezer + 0.005*fridge + 0.004*pavilion + 0.004*home + 0.004*pcs + 0.003*freezers + 0.003*workstation + 0.003*cartridges + 0.003*supplies + 0.003*nonstar + 0.003*appliances + 0.003*ink + 0.003*energy + 0.003*refrigerator + 0.003*zanussi + 0.002*probook'),\n",
       " (12,\n",
       "  '0.027*hp + 0.015*pc + 0.009*series + 0.009*ink + 0.008*notebook + 0.007*cartridges + 0.005*supplies + 0.005*pavilion + 0.005*pcs + 0.004*workstation + 0.003*home + 0.003*printer + 0.003*philips + 0.003*probook + 0.003*desktop + 0.002*compaq + 0.002*elitebook + 0.002*star + 0.002*cartridge + 0.002*consumer'),\n",
       " (13,\n",
       "  '0.026*hp + 0.016*pc + 0.009*series + 0.009*notebook + 0.006*pavilion + 0.005*pcs + 0.004*home + 0.004*workstation + 0.003*desktop + 0.003*star + 0.003*probook + 0.003*philips + 0.003*compaq + 0.002*consumer + 0.002*energy + 0.002*tv + 0.002*cartridges + 0.002*elitebook + 0.002*workstations + 0.002*supplies'),\n",
       " (14,\n",
       "  '0.018*hp + 0.010*pc + 0.008*series + 0.006*storage + 0.005*drives + 0.005*notebook + 0.004*flash + 0.004*memory + 0.004*drive + 0.004*energy + 0.003*workstation + 0.003*pavilion + 0.003*cards + 0.003*cables + 0.003*cclass + 0.003*home + 0.003*philips + 0.003*pcs + 0.003*consumer + 0.003*wd'),\n",
       " (15,\n",
       "  '0.028*hp + 0.018*pc + 0.010*series + 0.010*notebook + 0.006*home + 0.006*pavilion + 0.006*pcs + 0.004*philips + 0.004*tv + 0.004*workstation + 0.003*desktop + 0.003*consumer + 0.003*probook + 0.003*compaq + 0.003*elitebook + 0.002*entertainment + 0.002*energy + 0.002*workstations + 0.002*star + 0.002*monitors'),\n",
       " (16,\n",
       "  '0.028*hp + 0.017*pc + 0.010*series + 0.009*notebook + 0.006*workstation + 0.005*pavilion + 0.005*pcs + 0.004*home + 0.003*probook + 0.003*desktop + 0.003*elitebook + 0.003*workstations + 0.003*printer + 0.002*philips + 0.002*compaq + 0.002*tv + 0.002*business + 0.002*energy + 0.002*star + 0.002*supplies'),\n",
       " (17,\n",
       "  '0.022*hp + 0.015*pc + 0.008*series + 0.008*notebook + 0.006*monitors + 0.005*pavilion + 0.005*home + 0.005*philips + 0.004*tv + 0.004*pcs + 0.004*consumer + 0.003*workstation + 0.003*compaq + 0.003*products + 0.003*headphones + 0.002*desktop + 0.002*entertainment + 0.002*probook + 0.002*accessories + 0.002*monitor'),\n",
       " (18,\n",
       "  '0.025*hp + 0.015*pc + 0.009*series + 0.008*notebook + 0.006*pavilion + 0.004*pcs + 0.004*home + 0.003*workstation + 0.003*philips + 0.003*tv + 0.003*compaq + 0.003*consumer + 0.003*desktop + 0.002*headphones + 0.002*probook + 0.002*entertainment + 0.002*business + 0.002*cartridges + 0.002*ink + 0.002*energy'),\n",
       " (19,\n",
       "  '0.024*hp + 0.013*pc + 0.007*series + 0.007*notebook + 0.004*support + 0.004*pavilion + 0.004*business + 0.004*day + 0.004*pcs + 0.004*post + 0.003*year + 0.003*warranty + 0.003*hardware + 0.003*home + 0.003*philips + 0.002*workstation + 0.002*consumer + 0.002*desktop + 0.002*compaq + 0.002*probook')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find out how many topics exist in the doc\n",
    "hdp_model= models.HdpModel(corpus=corpusn, id2word=dictionary)\n",
    "hdp_model.show_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
