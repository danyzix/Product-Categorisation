{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd, mysql.connector\n",
    "import xgboost, textblob, string\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import keras\n",
    "from keras.layers import Embedding, Flatten, Dense, GlobalMaxPool1D, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('\\\\Users\\\\Dan\\\\Documents\\\\Python\\\\Data\\\\Flix_products\\\\SamsungReport.xlsx','Features', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>title</th>\n",
       "      <th>html</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>628714</td>\n",
       "      <td>xbox-supertemplate</td>\n",
       "      <td>xbox-supertemplate1111 kieskrreurig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>653143</td>\n",
       "      <td>contrast enhancer</td>\n",
       "      <td>Applies one million criteria for contrast yet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>653143</td>\n",
       "      <td>colour optimiser</td>\n",
       "      <td>Preserves the original level of brightness whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>653143</td>\n",
       "      <td>detail enhancer</td>\n",
       "      <td>Selectively sharpens edges so that you see thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>653143</td>\n",
       "      <td>3way burn-in protection</td>\n",
       "      <td>3 Types of Anti-Burn Program\\nScreen burn-in i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                    title  \\\n",
       "0      628714       xbox-supertemplate   \n",
       "1      653143        contrast enhancer   \n",
       "2      653143         colour optimiser   \n",
       "3      653143          detail enhancer   \n",
       "4      653143  3way burn-in protection   \n",
       "\n",
       "                                                html  \n",
       "0                xbox-supertemplate1111 kieskrreurig  \n",
       "1  Applies one million criteria for contrast yet ...  \n",
       "2  Preserves the original level of brightness whi...  \n",
       "3  Selectively sharpens edges so that you see thi...  \n",
       "4  3 Types of Anti-Burn Program\\nScreen burn-in i...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[['title', 'html']] = df1[['title', 'html']].astype(str)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id    49215\n",
       "title         49215\n",
       "html          49215\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.apply(lambda x: x.count(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate title and html for the same product_id into one cell\n",
    "df2 = df1.groupby('product_id')['title','html'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "df2['texts'] = df2['title'] + \" \" + df2['html']\n",
    "df3 = df2.groupby('product_id')['texts'].agg(lambda x: ' '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6036, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>628714</td>\n",
       "      <td>xbox-supertemplate xbox-supertemplate1111 kies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>653143</td>\n",
       "      <td>contrast enhancer colour optimiser detail enha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                                              texts\n",
       "0      628714  xbox-supertemplate xbox-supertemplate1111 kies...\n",
       "1      653143  contrast enhancer colour optimiser detail enha..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dan\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# merge some product details\n",
    "products = pd.read_csv('\\\\Users\\\\Dan\\\\Documents\\\\Python\\\\Data\\\\products.csv', sep='\\t')\n",
    "df3 = pd.merge(df3, products[['product_id', 'flix_parent_category', 'flix_subCategory1']],\n",
    "                 on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flix_parent_category\n",
       "Cameras                      276\n",
       "Computing/Gaming            1442\n",
       "Health & Beauty                2\n",
       "Home Appliances              703\n",
       "Home Entertainment          1076\n",
       "Office Needs                   4\n",
       "Other                          7\n",
       "Phone/Mobiles               1489\n",
       "Small Gadgets/Appliances      21\n",
       "nan                         1016\n",
       "Name: product_id, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.astype(str).groupby('flix_parent_category')['product_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop products with unassigned categories\n",
    "df3.dropna(subset=['flix_parent_category'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5020,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate description and product categories\n",
    "texts = df3['texts']\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100   # cuts off after 100 words\n",
    "training_samples = 3000    # trains on 200 samples\n",
    "#validation_samples = 10000   #  validates on 10,000 samples\n",
    "max_words = 1000    # considers only the top 10,000 words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts each description to a list of integers considering only the max_words\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)   # each  tokenized review is cut to maxlen (100 words or padded 0 if less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15497 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the target labels\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(df3['flix_parent_category'])\n",
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5020, 9)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = keras.utils.to_categorical(encoded_labels)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlb = MultiLabelBinarizer()\n",
    "# labels = mlb.fit_transform(encoded_labels)\n",
    "# labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (5020, 100)\n",
      "Shape of label tensor: (5020, 9)\n"
     ]
    }
   ],
   "source": [
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])  \n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: ]\n",
    "y_val = labels[training_samples: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(y_val), type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 100, 100)          100000    \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                640064    \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 740,649\n",
      "Trainable params: 740,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Training the same model without pretrained word embeddings\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3000 samples, validate on 2020 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 3s 860us/step - loss: 1.6137 - acc: 0.2777 - val_loss: 1.5468 - val_acc: 0.3064\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 2s 548us/step - loss: 1.4029 - acc: 0.4350 - val_loss: 1.6136 - val_acc: 0.2802\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 2s 547us/step - loss: 1.2013 - acc: 0.5490 - val_loss: 1.7511 - val_acc: 0.2668\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 2s 543us/step - loss: 1.0177 - acc: 0.6113 - val_loss: 1.8968 - val_acc: 0.2490\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 2s 545us/step - loss: 0.8997 - acc: 0.6287 - val_loss: 2.0403 - val_acc: 0.2540\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 2s 544us/step - loss: 0.8367 - acc: 0.6420 - val_loss: 2.1888 - val_acc: 0.2599\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 2s 558us/step - loss: 0.8007 - acc: 0.6420 - val_loss: 2.2784 - val_acc: 0.2475\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 2s 549us/step - loss: 0.7744 - acc: 0.6407 - val_loss: 2.3088 - val_acc: 0.2450\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 2s 547us/step - loss: 0.7530 - acc: 0.6427 - val_loss: 2.3417 - val_acc: 0.2381\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 2s 553us/step - loss: 0.7331 - acc: 0.6443 - val_loss: 2.3976 - val_acc: 0.2431\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "model.save_weights('from scratch_model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
